{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this embedding exercise, we are using the same dataset that we are using for our clustering exercise. This is comprised of movie information, and we will be treating the corpus as the culmination of the `Storyline` column from the csv DataFrame of the movie data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following bash command downloads the movie data from my hosted dropbox, and unzips it while ignoring the `Links` folder present in the zip file. After unzipping it, it removes the zip file, and moves the unzipped data into the `data` folder.\n",
    "\n",
    "*You'll notice the `|| true` at the end of each command, this is to ignore the exit code*\n",
    "\n",
    "- `curl`: fetches the data from the provided url\n",
    "    - `-L` flag follows redirects\n",
    "    - `-o` sets what to name the downloaded file, in this case it is `moviedata.zip`\n",
    "\n",
    "- `unzip`: unzips a `.zip` archive file\n",
    "    - `-o` sets what to name the unzipped file\n",
    "    - `-x` chooses what parts of the unzipped archive to ignore when processing\n",
    "\n",
    "`rm`: removes the specified file\n",
    "    - `-r` recursively removes the files from the specified artifact (object)\n",
    "    - `-f` forcibly removes the file, ignoring prompting the user to confirm deletion\n",
    "\n",
    "`mv`: moves the file to the specified folder, can be used to rename the file as well.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100    17  100    17    0     0     19      0 --:--:-- --:--:-- --:--:--    19    0     0     19      0 --:--:-- --:--:-- --:--:--    19\n",
      "100   496    0   496    0     0    267      0 --:--:--  0:00:01 --:--:--   267    0     0    267      0 --:--:--  0:00:01 --:--:--     0\n",
      "100 2488k  100 2488k    0     0   547k      0  0:00:04  0:00:04 --:--:--  986k\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  moviedata.zip\n",
      "  inflating: IMDb 2024 Movies TV Shows.csv  \n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "! curl -L -o moviedata.zip \"https://www.dropbox.com/scl/fi/9oku0kqcgakunde7n11xz/imdbmovies.zip?rlkey=1j0xygn3y4niywq4pu55fhapo&st=u7qyoch9&dl=1\" || true\n",
    "! unzip -o moviedata.zip -x \"Links/*\" || true\n",
    "! rm -rf moviedata.zip || true\n",
    "! mv \"IMDb 2024 Movies TV Shows.csv\" ../data/moviedata.csv || true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Necessary imports to embed the words from the corpus. Saves time coding boiler-plate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.typing as npt\n",
    "import pandas as pd\n",
    "import numpy.linalg as npl\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Budget', 'Home_Page', 'Movie_Name', 'Genres', 'Overview', 'Cast',\n",
       "       'Original_Language', 'Storyline', 'Production_Company', 'Release_Date',\n",
       "       'Revenue', 'Run_Time', 'Tagline', 'Vote_Average', 'Vote_Count'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reads in the csv file into DataFrame, which is useful for doing matrix operations\n",
    "movie_data = pd.read_csv(\"../data/moviedata.csv\")\n",
    "\n",
    "movie_data.columns # Just printing out the columns, so we know which columns to target to form our corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19793 7804 10831 4846\n"
     ]
    }
   ],
   "source": [
    "common_words = set(['a', 'at', 'the', 'then', 'is', 'of', 'and', 'with', 'as', 'to', 'for', 'an', 'in', 'this', 'not', 'be'])\n",
    "\n",
    "storyline_corpus = list(filter(lambda x: x not in common_words, movie_data['Storyline'].str.cat(sep=' ').lower().split(' '))) \n",
    "storyline_vocabulary = set(storyline_corpus)\n",
    "\n",
    "overview_corpus = list(filter(lambda x: x not in common_words, movie_data['Overview'].str.cat(sep=' ').lower().split(' ')))\n",
    "overview_vocabulary = set(overview_corpus)\n",
    "\n",
    "print(len(storyline_corpus), len(storyline_vocabulary), len(overview_corpus), len(overview_vocabulary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So as you can see, required a little wrangling to get a nice corpus. We have a total of 19,793 words in the corpus. The vocabulary is the set of unique words in the corpus. It is significantly less then the corpus, but still very high when considering what techniques we will utilize."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Symbol-Based Representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.notion.so/cthacker/Embedding-1b537d6ae5d3807bae75f57e1ddfe128?pvs=97#1b537d6ae5d3804abe7af27831c45da3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-Hot Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One-Hot Encoding is a symbol-based representation, that is, it takes words and embeds them into the euclidean space. Therefore in this usage, it is a word-embedding algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_encoding = {}\n",
    "\n",
    "for ind, each_word in enumerate(storyline_vocabulary):\n",
    "    one_hot_encoding[each_word] = np.zeros((len(storyline_vocabulary,)))\n",
    "    one_hot_encoding[each_word][ind] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This computes a matrix of the *cosine similarity values* between the One-Hot encoded words.\n",
    "> Remember that the One-Hot encoding is a *word-embedding* algorithm, it maps the words from the theoretical dictionary space, into the euclidean space. If we are to analyze if the One-Hot encoding algorithm preserved the original structure of the space, we can look for cosine similarities among words (their distance away from each-other).\n",
    "\n",
    "But since the One-Hot encoding basically maps all words to a unit vector pointing to a dimension. The distances among all the words will be the same, which isn't an accurate preservation of the original space, because in the original space we would see varying distances among words. Closeness among words that are closely related, and far distance among words that are not related, and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This computes a matrix of the cosine similarity values between the One-Hot encoded words.\n",
    "\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    return (np.dot(vec1, vec2) / (npl.norm(vec1) * npl.norm(vec2)))\n",
    "\n",
    "cosine_similarity_matrix = np.zeros((len(one_hot_encoding.values()), len(one_hot_encoding.values())))\n",
    "for word1ind, (word1key, word1value) in enumerate(one_hot_encoding.items()):\n",
    "    for word2ind, (word2key, word2value) in enumerate(one_hot_encoding.items()):\n",
    "        if word1key == word2key:\n",
    "            cosine_similarity_matrix[word1ind][word2ind] = 1.0\n",
    "            continue\n",
    "\n",
    "        if word2ind < word1ind:\n",
    "            continue\n",
    "\n",
    "        cosine_similarity_matrix[word1ind][word2ind] = cosine_similarity(word1value, word2value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], shape=(7804, 7804))"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag-of-Words (BoW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'storyline': array([1., 3., 2., ..., 1., 5., 0.], shape=(8985,)),\n",
       " 'overview': array([1., 1., 0., ..., 1., 2., 1.], shape=(8985,))}"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "bag_of_words = {}\n",
    "corpus_information = {\n",
    "    'storyline_corpus': {\n",
    "        'corpus': storyline_corpus,\n",
    "        'vocabulary':storyline_vocabulary,\n",
    "    },\n",
    "    'overview_corpus': {\n",
    "        'corpus': overview_corpus,\n",
    "        'vocabulary': overview_vocabulary\n",
    "    }\n",
    "}\n",
    "\n",
    "# This is a common technique among bag-of-words analysis, this allows us to ensure that both vectorized documents have similar shapes\n",
    "# which will allow us to perform dimension structure preservation analysis without shaping the vectors.\n",
    "combined_vocabulary = corpus_information['storyline_corpus']['vocabulary'].union(corpus_information['overview_corpus']['vocabulary'])\n",
    "corpuses_keys = ['storyline', 'overview']\n",
    "\n",
    "for each_corpus_key in corpuses_keys:\n",
    "    found_corpus: list[str] = corpus_information[f'{each_corpus_key}_corpus']['corpus']\n",
    "    bag_of_words[each_corpus_key] = np.zeros((len(combined_vocabulary))) # initializes a 1d (vector) because we aren't \"specifying\" the second dimension, therefore it assumes we want a 1D array, which is basically a vector.\n",
    "    corpus_word_count = Counter(found_corpus)\n",
    "\n",
    "    for vocab_word_ind, each_vocab_word in enumerate(combined_vocabulary):\n",
    "        bag_of_words[each_corpus_key][vocab_word_ind] = corpus_word_count.get(each_vocab_word) or 0\n",
    "\n",
    "bag_of_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Related'"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_of_words_similarity = cosine_similarity(bag_of_words['storyline'], bag_of_words['overview'])\n",
    "\n",
    "'Related' if bag_of_words_similarity > 0 else 'Unrelated' if bag_of_words_similarity == 0 else 'Not Related'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
