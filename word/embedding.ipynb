{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this embedding exercise, we are using the same dataset that we are using for our clustering exercise. This is comprised of movie information, and we will be treating the corpus as the culmination of the `Storyline` column from the csv DataFrame of the movie data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following bash command downloads the movie data from my hosted dropbox, and unzips it while ignoring the `Links` folder present in the zip file. After unzipping it, it removes the zip file, and moves the unzipped data into the `data` folder.\n",
    "\n",
    "*You'll notice the `|| true` at the end of each command, this is to ignore the exit code*\n",
    "\n",
    "- `curl`: fetches the data from the provided url\n",
    "    - `-L` flag follows redirects\n",
    "    - `-o` sets what to name the downloaded file, in this case it is `moviedata.zip`\n",
    "\n",
    "- `unzip`: unzips a `.zip` archive file\n",
    "    - `-o` sets what to name the unzipped file\n",
    "    - `-x` chooses what parts of the unzipped archive to ignore when processing\n",
    "\n",
    "`rm`: removes the specified file\n",
    "    - `-r` recursively removes the files from the specified artifact (object)\n",
    "    - `-f` forcibly removes the file, ignoring prompting the user to confirm deletion\n",
    "\n",
    "`mv`: moves the file to the specified folder, can be used to rename the file as well.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100    17  100    17    0     0     31      0 --:--:-- --:--:-- --:--:--    31\n",
      "100   496    0   496    0     0    621      0 --:--:-- --:--:-- --:--:--   621\n",
      "100 2488k  100 2488k    0     0  2095k      0  0:00:01  0:00:01 --:--:-- 13.0M\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  moviedata.zip\n",
      "  inflating: IMDb 2024 Movies TV Shows.csv  \n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: tqdm in /home/cthacker/.local/lib/python3.10/site-packages (4.67.1)\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "! curl -L -o moviedata.zip \"https://www.dropbox.com/scl/fi/9oku0kqcgakunde7n11xz/imdbmovies.zip?rlkey=1j0xygn3y4niywq4pu55fhapo&st=v86gdypi&dl=1\" || true\n",
    "! unzip -o moviedata.zip -x \"Links/*\" || true\n",
    "! rm -rf moviedata.zip || true\n",
    "! mv \"IMDb 2024 Movies TV Shows.csv\" ../data/moviedata.csv || true\n",
    "! pip install tqdm || true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Necessary imports to embed the words from the corpus. Saves time coding boiler-plate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.typing as npt\n",
    "import pandas as pd\n",
    "import numpy.linalg as npl\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Budget', 'Home_Page', 'Movie_Name', 'Genres', 'Overview', 'Cast',\n",
       "       'Original_Language', 'Storyline', 'Production_Company', 'Release_Date',\n",
       "       'Revenue', 'Run_Time', 'Tagline', 'Vote_Average', 'Vote_Count'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reads in the csv file into DataFrame, which is useful for doing matrix operations\n",
    "movie_data = pd.read_csv(\"../data/moviedata.csv\")\n",
    "\n",
    "movie_data.columns # Just printing out the columns, so we know which columns to target to form our corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19757 6512 10821 4239\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "common_words = set(['a', 'at', 'the', 'then', 'is', 'of', 'and', 'with', 'as', 'to', 'for', 'an', 'in', 'this', 'not', 'be'])\n",
    "replace_punctuation = r'[.,;:\\[\\]{}()&*%^#$!@?\"]+'\n",
    "\n",
    "storyline_corpus = list(filter(lambda x: x not in common_words, re.sub(replace_punctuation, '', movie_data['Storyline'].str.cat(sep=' ')).lower().split(' '))) \n",
    "storyline_vocabulary = set(storyline_corpus)\n",
    "\n",
    "overview_corpus = list(filter(lambda x: x not in common_words, re.sub(replace_punctuation, '', movie_data['Overview'].str.cat(sep=' ')).lower().split(' ')))\n",
    "overview_vocabulary = set(overview_corpus)\n",
    "\n",
    "print(len(storyline_corpus), len(storyline_vocabulary), len(overview_corpus), len(overview_vocabulary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So as you can see, required a little wrangling to get a nice corpus. We have a total of 19,793 words in the corpus. The vocabulary is the set of unique words in the corpus. It is significantly less then the corpus, but still very high when considering what techniques we will utilize."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Symbol-Based Representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.notion.so/cthacker/Embedding-1b537d6ae5d3807bae75f57e1ddfe128?pvs=97#1b537d6ae5d3804abe7af27831c45da3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-Hot Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One-Hot Encoding is a symbol-based representation, that is, it takes words and embeds them into the euclidean space. Therefore in this usage, it is a word-embedding algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing one-hot encoding for `storyline_vocabulary`: 100%|██████████| 6512/6512 [00:00<00:00, 70627.57it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 1., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 1., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 1., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 1., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 1.]], shape=(6512, 6512))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Each row represents the word, and the columns are the one-hot encoding vector.\n",
    "one_hot_encoding_matrix = np.zeros((len(storyline_vocabulary), len(storyline_vocabulary))) # (num_words x num_words) \n",
    "\n",
    "for ind, each_word in tqdm(enumerate(storyline_vocabulary), total = len(storyline_vocabulary), desc = 'Computing one-hot encoding for `storyline_vocabulary`'):\n",
    "\n",
    "    # construct the one-hot encoding vector\n",
    "    one_hot_encoding_matrix_vec = np.zeros((1, len(storyline_vocabulary)))\n",
    "    one_hot_encoding_matrix_vec[0][ind] = 1\n",
    "\n",
    "    # set the word to the computed one-hot encoding vector\n",
    "    one_hot_encoding_matrix[ind] = one_hot_encoding_matrix_vec\n",
    "\n",
    "one_hot_encoding_matrix # Is the identity matrix! I^n where n is the # of words in the vocabulary. This allows us to fetch the corresponding one-hot matrix for a given word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This computes a matrix of the *cosine similarity values* between the One-Hot encoded words.\n",
    "> Remember that the One-Hot encoding is a *word-embedding* algorithm, it maps the words from the theoretical dictionary space, into the euclidean space. If we are to analyze if the One-Hot encoding algorithm preserved the original structure of the space, we can look for cosine similarities among words (their distance away from each-other).\n",
    "\n",
    "But since the One-Hot encoding basically maps all words to a unit vector pointing to a dimension. The distances among all the words will be the same, which isn't an accurate preservation of the original space, because in the original space we would see varying distances among words. Closeness among words that are closely related, and far distance among words that are not related, and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(vec1, vec2):\n",
    "    return (np.dot(vec1, vec2) / (npl.norm(vec1) * npl.norm(vec2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'storyline_vocabulary' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m## This computes a matrix of the cosine similarity values between the One-Hot encoded words.\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m storyline_vocab_words \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[43mstoryline_vocabulary\u001b[49m)\n\u001b[0;32m      4\u001b[0m cosine_similarity_matrix \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(one_hot_encoding_matrix\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m word1ind, word1 \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28menumerate\u001b[39m(storyline_vocabulary), total \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(storyline_vocabulary), desc \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mComputing cosine similarity for `storyline_vocabulary`\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "\u001b[1;31mNameError\u001b[0m: name 'storyline_vocabulary' is not defined"
     ]
    }
   ],
   "source": [
    "## This computes a matrix of the cosine similarity values between the One-Hot encoded words.\n",
    "\n",
    "storyline_vocab_words = list(storyline_vocabulary)\n",
    "cosine_similarity_matrix = np.zeros(one_hot_encoding_matrix.shape)\n",
    "for word1ind, word1 in tqdm(enumerate(storyline_vocabulary), total = len(storyline_vocabulary), desc = \"Computing cosine similarity for `storyline_vocabulary`\"):\n",
    "    for word2ind in range(word1ind, len(storyline_vocabulary)):\n",
    "        word2 = storyline_vocab_words[word2ind]\n",
    "        if word1 == word2:\n",
    "            cosine_similarity_matrix[word1ind][word2ind] = 1.0\n",
    "            continue\n",
    "\n",
    "        if word2ind < word1ind:\n",
    "            continue\n",
    "\n",
    "        cosine_similarity_matrix[word1ind][word2ind] = cosine_similarity(one_hot_encoding_matrix[word1ind], one_hot_encoding_matrix[word2ind])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 1., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 1., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 1., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 1., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 1.]], shape=(7804, 7804))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag-of-Words (BoW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'storyline': array([ 0.,  2.,  1., ..., 15.,  1.,  2.], shape=(8985,)),\n",
       " 'overview': array([ 1.,  0.,  0., ..., 13.,  1.,  1.], shape=(8985,))}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "bag_of_words = {}\n",
    "corpus_information = {\n",
    "    'storyline_corpus': {\n",
    "        'corpus': storyline_corpus,\n",
    "        'vocabulary':storyline_vocabulary,\n",
    "    },\n",
    "    'overview_corpus': {\n",
    "        'corpus': overview_corpus,\n",
    "        'vocabulary': overview_vocabulary\n",
    "    }\n",
    "}\n",
    "\n",
    "corpora = [storyline_corpus, overview_corpus]\n",
    "\n",
    "# This is a common technique among bag-of-words analysis, this allows us to ensure that both vectorized documents have similar shapes\n",
    "# which will allow us to perform dimension structure preservation analysis without shaping the vectors.\n",
    "combined_vocabulary = corpus_information['storyline_corpus']['vocabulary'].union(corpus_information['overview_corpus']['vocabulary'])\n",
    "corpuses_keys = ['storyline', 'overview']\n",
    "\n",
    "for each_corpus_key in corpuses_keys:\n",
    "    found_corpus: list[str] = corpus_information[f'{each_corpus_key}_corpus']['corpus']\n",
    "    bag_of_words[each_corpus_key] = np.zeros((len(combined_vocabulary))) # initializes a 1d (vector) because we aren't \"specifying\" the second dimension, therefore it assumes we want a 1D array, which is basically a vector.\n",
    "    corpus_word_count = Counter(found_corpus)\n",
    "\n",
    "    for vocab_word_ind, each_vocab_word in enumerate(combined_vocabulary):\n",
    "        bag_of_words[each_corpus_key][vocab_word_ind] = corpus_word_count.get(each_vocab_word) or 0\n",
    "\n",
    "bag_of_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Related'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_of_words_similarity = cosine_similarity(bag_of_words['storyline'], bag_of_words['overview'])\n",
    "\n",
    "'Related' if bag_of_words_similarity > 0 else 'Unrelated' if bag_of_words_similarity == 0 else 'Not Related'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is another symbol-based representation technique we can utilize as well, called TF-IDF, which stands for *Term Frequency-Inverse Document Frequency*, what it essentially does it measure the importance of specific terms in specific documents. Let's say that we have a vocabulary of *n* terms and a corpora of *m* documents, then the resulting matrix will be size m x n. Where the rows correspond to the corpus (document) and the columns correspond to the importance of the word in that given corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a few formulas we must first establish. The first one is the measurement of the importance of the term (word) in a given corpus. This is simply just an average calculation.\n",
    "\n",
    "**Term-Frequency (local importance)**\n",
    "\n",
    "$$\n",
    "TF(t, d) = \\frac{f_{t, d}}{\\sum_{w \\in D}f_{w,d}}\n",
    "$$\n",
    "\n",
    "Where $f_{t,d}$ is the frequency of the term (word) $t$ in the document (corpus) $d$\n",
    "\n",
    "- This formula effectively removes the bias of longer documents containing the word more, by applying the summation (basically the # of words in the document) in the denominator, which allows for the frequency of the singular term to be scaled by the number of words in the document, which is an effective way of removing longer document bias.\n",
    "    - \"Does TF completely remove bias?\" -> Not quite, it partially removes bias, but the application of IDF to the total calculation, diminishes the impact of common words, therefore removing bias of common words appearing frequently in documents as well.\n",
    "\n",
    "- This is the first measurement we will utilize in computing the total value of TF-IDF for a given term and an entire corpora, the next measurement is the *Inverse-Document Frequency* formula.\n",
    "\n",
    "- The formula is 2-parts, first part is *local importance* (that is importance among singular documents), while the second part is *global importance* (that is importance across **all** documents)\n",
    "\n",
    "**Inverse-Document Frequency (global importance)**\n",
    "\n",
    "$$\n",
    "    IDF(t, D) = \\log\\left(\\frac{N}{1 + DF(t)}\\right)\n",
    "$$\n",
    "\n",
    "- $t$ is the singular word\n",
    "- $D$ is the entire corpora\n",
    "- $N$ is the # of documents in the corpora\n",
    "- $DF$ is the frequency of *documents* that contain the term *t*, not to be confused with the # of times the term $t$ appears in documents, it is actually the # of documents that contain the term $t$\n",
    "\n",
    "> Why use $log$?\n",
    "\n",
    "The idea behind using $\\log$ lies in the normalization of the values, but more **importantly** it lies in the idea that we want a smoother scaling of the values calculated. If we don't apply log, the values can get large extremely fast, assuming we are working with not just 10 documents, but potentially thousands, tens of thousands, etc. We want that value to be stable, wrapping the calculation in $\\log$ allows us to have $\\log$ dominate the growth of the term.\n",
    "\n",
    "> Why aren't we dividing by the total # of documents, to get an accurate average of the frequency of the word across all documents (corpora)?\n",
    "\n",
    "That lies in the idea about why the formula is called **inverse** document frequency. Essentially, when the document appears in tons of documents, the denominator grows, which mean the calculated value shrinks. If the word appears in very little documents, the denominator shrinks, which means the calculated value grows. Therefore, we are putting more importance on words that are considered *rare* across the documents, and putting less importance on common words.\n",
    "\n",
    "> Why is it called \"inverse\" document frequency?\n",
    "\n",
    "If we wanted to compute the frequency of the word across all documents, we would essentially just flip the fraction, and get the probability of the word appearing in all documents. If we want to **invert** that, and compute the probability of the word *not* appearing in documents (*rareness* essentially), we just invert the fraction, and voila!\n",
    "\n",
    "*Combining these two formulas (local importance and global importance), we can finalize the result with the following formula for TF-IDF*:\n",
    "\n",
    "$$\n",
    "\\text{TF-IDF} (t, d, D) = TF(t, d)  \\times IDF(t, D)\n",
    "$$\n",
    "$$\n",
    "\\textbf{OR}\n",
    "$$\n",
    "$$\n",
    "\\text{TF-IDF} (t,d,D) =\\frac{f_{t, d}}{\\sum_{w \\in D}f_{w,d}} \\times \\log\\left(\\frac{N}{1 + DF(t)}\\right)\n",
    "$$\n",
    "\n",
    "> What is the reason for the multiplication?\n",
    "\n",
    "The reason for the multiplication is that we want to merge the frequencies on a *proportional* level, rather than just shifting the value down or up. Multiplying the two frequencies allows the values to interact on a more meaningful level than adding the values together, which implies the values have equal impact. The multiplication scales the impact respectively to the IDF or TF part of the formula."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('sympathising', 0.0, 0.0)"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import List\n",
    "import math\n",
    "import random\n",
    "\n",
    "def term_frequency(term: str, corpus: List[str]):\n",
    "    corpus_counter = Counter(corpus)\n",
    "    return corpus_counter.get(term, 0) / sum(list(corpus_counter.values()))\n",
    "\n",
    "def word_in_corpus(term: str, corpus: List[str]):\n",
    "    return term in set(corpus)\n",
    "\n",
    "def inverse_document_frequency(term: str, corpora: List[List[str]]):\n",
    "    return math.log(len(corpora) / (1 + sum([1 if word_in_corpus(term, each_corpus) else 0 for each_corpus in corpora])))\n",
    "\n",
    "def term_frequency_inverse_document_frequency(term: str, corpus: List[str], corpora: List[List[str]]) -> float:\n",
    "    return term_frequency(term, corpus) * inverse_document_frequency(term, corpora)\n",
    "\n",
    "\n",
    "\n",
    "random_word = random.choice(list(combined_vocabulary))\n",
    "tf_idf_storyline = term_frequency_inverse_document_frequency(random_word, storyline_corpus, corpora)\n",
    "tf_idf_overview = term_frequency_inverse_document_frequency(random_word, storyline_corpus, corpora)\n",
    "\n",
    "random_word, tf_idf_storyline, tf_idf_overview"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
